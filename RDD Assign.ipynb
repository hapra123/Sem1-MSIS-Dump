{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNHyRVaAjoyzMjbRF3bwh2Q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NWWZR60Iyo0Q","executionInfo":{"status":"ok","timestamp":1727932379844,"user_tz":-330,"elapsed":51612,"user":{"displayName":"Smayan Hegde","userId":"00330384273200570673"}},"outputId":"c766e316-b59d-47c8-a811-078fb61870f3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyspark\n","  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840625 sha256=6e742aeb327449886576d8220b3fcb80bd5bd2d086c2dd4272426e65a8993d88\n","  Stored in directory: /root/.cache/pip/wheels/1b/3a/92/28b93e2fbfdbb07509ca4d6f50c5e407f48dce4ddbda69a4ab\n","Successfully built pyspark\n","Installing collected packages: pyspark\n","Successfully installed pyspark-3.5.3\n"]}],"source":["!pip install pyspark"]},{"cell_type":"markdown","source":["ABD ASSIGNMENT\n"],"metadata":{"id":"a-XoIXbc2WNr"}},{"cell_type":"markdown","source":["1. Creating RDDs in three different ways"],"metadata":{"id":"MZYkvc6c0XQw"}},{"cell_type":"code","source":["from pyspark import SparkContext\n","\n","# Method 1: Creating RDD from a parallelized collection\n","rdd1 = sc.parallelize([1, 2, 3, 4, 5])\n","\n","# Method 2: Creating RDD from an external text file\n","rdd2 = sc.textFile(\"/content/input.txt\")\n","\n","# Method 3: Creating RDD from existing RDD (Transformation)\n","rdd3 = rdd1.map(lambda x: x * 2)  # Doubling the elements of rdd1\n","\n","# Display the RDDs\n","print(rdd1.collect())  # Output: [1, 2, 3, 4, 5]\n","print(rdd2.collect())  # Output: The contents of the file\n","print(rdd3.collect())  # Output: [2, 4, 6, 8, 10]\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VyUpXEPG1D_i","executionInfo":{"status":"ok","timestamp":1727933109747,"user_tz":-330,"elapsed":1407,"user":{"displayName":"Smayan Hegde","userId":"00330384273200570673"}},"outputId":"28138e1d-9099-4743-c846-baa82d0368e0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[1, 2, 3, 4, 5]\n","['Project Jupyter’s tools are available for installation via the Python Package Index, the leading repository of software created for the Python programming language.', '', 'This page uses instructions with pip, the recommended installation tool for Python. If you require environment management as opposed to just installation, look into conda, mamba, pipenv, and Homebrew.']\n","[2, 4, 6, 8, 10]\n"]}]},{"cell_type":"markdown","source":["2. Counting the number of words in a file using RDD operations"],"metadata":{"id":"vXvU7PvY1zu5"}},{"cell_type":"code","source":["# Read the text file\n","rdd_text = sc.textFile(\"/content/input.txt\")\n","\n","# Split each line into words\n","words_rdd = rdd_text.flatMap(lambda line: line.split(\" \"))\n","\n","# Count the words\n","word_count = words_rdd.count()\n","\n","print(f\"Number of words in the file: {word_count}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UcRaKfHa1oFX","executionInfo":{"status":"ok","timestamp":1727933160042,"user_tz":-330,"elapsed":962,"user":{"displayName":"Smayan Hegde","userId":"00330384273200570673"}},"outputId":"8e84dd9c-668b-4252-c9f3-a6e00223fa98"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of words in the file: 53\n"]}]},{"cell_type":"markdown","source":["3. Word Frequency in a given file"],"metadata":{"id":"QxE5or4w163n"}},{"cell_type":"code","source":["# Split each line into words\n","words_rdd = rdd_text.flatMap(lambda line: line.split(\" \"))\n","\n","# Create key-value pairs (word, 1)\n","word_pairs_rdd = words_rdd.map(lambda word: (word, 1))\n","\n","# Reduce by key (word) to get the frequency\n","word_frequency_rdd = word_pairs_rdd.reduceByKey(lambda a, b: a + b)\n","\n","# Collect and display the word frequencies\n","print(word_frequency_rdd.collect())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eN7NsAWj17Rn","executionInfo":{"status":"ok","timestamp":1727933191095,"user_tz":-330,"elapsed":930,"user":{"displayName":"Smayan Hegde","userId":"00330384273200570673"}},"outputId":"fb5f990f-334c-4a46-c40d-1f594d9c304d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[('Project', 1), ('Jupyter’s', 1), ('tools', 1), ('are', 1), ('installation', 2), ('via', 1), ('Python', 2), ('Package', 1), ('leading', 1), ('of', 1), ('created', 1), ('programming', 1), ('', 1), ('page', 1), ('uses', 1), ('instructions', 1), ('Python.', 1), ('management', 1), ('as', 1), ('just', 1), ('installation,', 1), ('look', 1), ('into', 1), ('conda,', 1), ('mamba,', 1), ('available', 1), ('for', 3), ('the', 4), ('Index,', 1), ('repository', 1), ('software', 1), ('language.', 1), ('This', 1), ('with', 1), ('pip,', 1), ('recommended', 1), ('tool', 1), ('If', 1), ('you', 1), ('require', 1), ('environment', 1), ('opposed', 1), ('to', 1), ('pipenv,', 1), ('and', 1), ('Homebrew.', 1)]\n"]}]},{"cell_type":"markdown","source":["4. Convert all words in a file to uppercase"],"metadata":{"id":"BhPGYZLG2ZbS"}},{"cell_type":"code","source":["from pyspark import SparkContext\n","\n","# Specify the path to the input text file\n","input_file = \"/content/input.txt\"\n","\n","# Read the text file into an RDD (one line per element)\n","text_rdd = sc.textFile(input_file)\n","\n","# Convert all words in the file to uppercase\n","uppercase_rdd = text_rdd.map(lambda line: line.upper())\n","\n","# Save the modified RDD to a new text file\n","uppercase_rdd.saveAsTextFile(\"path_to_uppercase.txt\")\n","\n","# Stop the SparkContext\n","sc.stop()\n"],"metadata":{"id":"6Asfb1uk2Zog"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" 5)Convert all words in a file to lowerrcase"],"metadata":{"id":"F2EuVnfa_AVD"}},{"cell_type":"code","source":["from pyspark import SparkContext\n","\n","# Initialize SparkContext\n","sc = SparkContext(\"local\", \"Convert Words to Lowercase\")\n","\n","# Specify the path to the input text file\n","input_file = \"/content/input.txt\"\n","\n","# Read the text file into an RDD (one line per element)\n","text_rdd = sc.textFile(input_file)\n","\n","# Convert all words in the file to lowercase\n","lowercase_rdd = text_rdd.map(lambda line: line.lower())\n","\n","# Save the modified RDD to a new text file\n","lowercase_rdd.saveAsTextFile(\"path_to_output_file.txt\")\n","\n","# Stop the SparkContext\n","sc.stop()\n"],"metadata":{"id":"ZPSYW6g5_D2C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["6)Write a program to capitalize first letter of each words in file (use string capitalize() method)."],"metadata":{"id":"ePhiMdctCrej"}},{"cell_type":"code","source":["from pyspark import SparkContext\n","\n","# Initialize SparkContext\n","sc = SparkContext(\"local\", \"Capitalize First Letter of Each Word\")\n","\n","# Specify the path to the input text file\n","input_file = \"/content/input.txt\"\n","\n","# Read the text file into an RDD (one line per element)\n","text_rdd = sc.textFile(input_file)\n","\n","# Capitalize the first letter of each word in the file\n","capitalized_rdd = text_rdd.map(lambda line: ' '.join(word.capitalize() for word in line.split()))\n","\n","# Save the modified RDD to a new text file\n","capitalized_rdd.saveAsTextFile(\"path_to_capitalize.txt\")\n","\n","# Stop the SparkContext\n","sc.stop()\n"],"metadata":{"id":"BgfmN0qTCt7I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["7. Find the number of occurrences of a specific word in a file"],"metadata":{"id":"DnHWBJul2Z4B"}},{"cell_type":"code","source":["# Word to search for\n","search_word = \"spark\"\n","\n","# Count occurrences of the word\n","word_occurrences = words_rdd.filter(lambda word: word == search_word).count()\n","\n","print(f\"The word '{search_word}' appears {word_occurrences} times in the file.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eUz0O7VX2aE4","executionInfo":{"status":"ok","timestamp":1727933344696,"user_tz":-330,"elapsed":924,"user":{"displayName":"Smayan Hegde","userId":"00330384273200570673"}},"outputId":"aeed91be-fa73-40c3-bf83-aa5b5b18d279"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The word 'spark' appears 0 times in the file.\n"]}]},{"cell_type":"markdown","source":["8)Select only the sentences containing given word from a text file."],"metadata":{"id":"CnQ3gsQwCvaJ"}},{"cell_type":"code","source":["from pyspark import SparkContext\n","\n","# Check if a SparkContext already exists\n","if not 'sc' in globals():\n","    # Initialize SparkContext\n","    sc = SparkContext(\"local\", \"Filter Sentences by Word\")\n","else:\n","    # Reuse the existing SparkContext\n","    print(\"Reusing existing SparkContext\")\n","\n","# Specify the path to the input text file\n","input_file = \"/content/input.txt\"\n","\n","# The word to search for\n","search_word = \"This\"\n","\n","# Read the text file into an RDD (one line per element)\n","text_rdd = sc.textFile(input_file)\n","\n","# Filter sentences that contain the search word (case-insensitive)\n","filtered_sentences = text_rdd.filter(lambda sentence: search_word.lower() in sentence.lower())\n","\n","# Save the filtered sentences to a new text file\n","filtered_sentences.saveAsTextFile(\"path_to_sentences.txt\")\n","\n","# Stop the SparkContext\n","# sc.stop() # Only stop the SparkContext at the end of the session or if you need to create a new one with different configurations"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dz1yQKiXC3YZ","executionInfo":{"status":"ok","timestamp":1727937135473,"user_tz":-330,"elapsed":2224,"user":{"displayName":"Smayan Hegde","userId":"00330384273200570673"}},"outputId":"3ade662c-8da7-424f-9fcf-900cfd9125b5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Reusing existing SparkContext\n"]}]},{"cell_type":"markdown","source":["9)Find the longest length of word from given set of words"],"metadata":{"id":"fYJZtcDb4ruU"}},{"cell_type":"code","source":["# Import the necessary libraries\n","from pyspark import SparkContext\n","\n","# Example: A list of words\n","words = [\"example\", \"spark\", \"resilient\", \"distributed\", \"dataset\"]\n","\n","# Create an RDD from the list of words\n","words_rdd = sc.parallelize(words)\n","\n","# Map each word to its length\n","word_lengths = words_rdd.map(lambda word: len(word))\n","\n","# Find the maximum length\n","longest_word_length = word_lengths.max()\n","\n","# Print the result\n","print(f\"The longest word length is: {longest_word_length}\")\n","\n","# Stop the SparkContext\n","sc.stop()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XGy064Tu4r6z","executionInfo":{"status":"ok","timestamp":1727934670679,"user_tz":-330,"elapsed":3000,"user":{"displayName":"Smayan Hegde","userId":"00330384273200570673"}},"outputId":"e19062dc-93ac-4cfb-8e3d-75c9aa50b21b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The longest word length is: 11\n"]}]},{"cell_type":"markdown","source":["10. Map registration numbers to corresponding branches"],"metadata":{"id":"WRRe1puN2piE"}},{"cell_type":"code","source":["# List of registration numbers\n","reg_numbers = [\"58001\", \"57034\", \"38021\", \"39045\", \"47065\"]\n","\n","# Define the mapping of registration series to branches\n","def map_to_branch(reg_number):\n","    if reg_number.startswith(\"58000\"):\n","        return (reg_number, \"BDA\")\n","    elif reg_number.startswith(\"57000\"):\n","        return (reg_number, \"AIML\")\n","    elif reg_number.startswith(\"38000\"):\n","        return (reg_number, \"VLSI\")\n","    elif reg_number.startswith(\"39000\"):\n","        return (reg_number, \"ES\")\n","    elif reg_number.startswith(\"47000\"):\n","        return (reg_number, \"CDC\")\n","    else:\n","        return (reg_number, \"Unknown\")\n","\n","# Create RDD and map to branch\n","reg_rdd = sc.parallelize(reg_numbers)\n","branch_rdd = reg_rdd.map(map_to_branch)\n","\n","# Display the registration number and corresponding branch\n","print(branch_rdd.collect())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EFW-kfkR2mVZ","executionInfo":{"status":"ok","timestamp":1727933373310,"user_tz":-330,"elapsed":1088,"user":{"displayName":"Smayan Hegde","userId":"00330384273200570673"}},"outputId":"5a776dbf-8229-4fff-a3ed-3a241cbff42b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[('58001', 'Unknown'), ('57034', 'Unknown'), ('38021', 'Unknown'), ('39045', 'Unknown'), ('47065', 'Unknown')]\n"]}]},{"cell_type":"markdown","source":["11. Find maximum, minimum, sum, and mean of numbers in a file"],"metadata":{"id":"oYDUeZHa23nC"}},{"cell_type":"code","source":["# Read the text file containing numbers (assume numbers are separated by spaces)\n","numbers_rdd = sc.textFile(\"/content/numbers.txt\").flatMap(lambda line: line.split())\n","\n","# Convert to integers\n","numbers_rdd = numbers_rdd.map(lambda x: int(x))\n","\n","# Find max, min, sum, and mean\n","max_number = numbers_rdd.max()\n","min_number = numbers_rdd.min()\n","sum_numbers = numbers_rdd.sum()\n","mean_number = sum_numbers / numbers_rdd.count()\n","\n","print(f\"Max: {max_number}, Min: {min_number}, Sum: {sum_numbers}, Mean: {mean_number}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wFwH1-Av24Lq","executionInfo":{"status":"ok","timestamp":1727933663939,"user_tz":-330,"elapsed":3330,"user":{"displayName":"Smayan Hegde","userId":"00330384273200570673"}},"outputId":"f7cf54a3-f2f8-4ed5-8d1f-0c2db7ca3e25"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Max: 98, Min: 1, Sum: 3441, Mean: 47.136986301369866\n"]}]},{"cell_type":"markdown","source":["12. A text file (citizen.txt) contains data about citizens of country. Fields (information in file) are Name, dob, Phone, email and state name. Another file contains mapping of state names to state code like Karnataka is codes as KA, TamilNadu as TN, Kerala KL etc. Compress the citizen.txt file by changing full state name to state code."],"metadata":{"id":"UhFJLWrH39Gs"}},{"source":["from pyspark import SparkContext\n","\n","   # Get the existing SparkContext or create a new one if none exists\n","sc = SparkContext.getOrCreate()\n","\n","   # Your code for creating and manipulating RDDs\n","rdd1 = sc.parallelize([1, 2, 3, 4, 5])\n","\n","   # Load citizen data from citizen.txt (assuming the fields are comma-separated)\n","citizen_file = \"/content/citizen.txt\"\n","citizen_rdd = sc.textFile(citizen_file)\n","\n","# Load the state mapping file (state name, state code)\n","state_mapping_file = \"/content/path_to_state_mapping.txt\"\n","state_mapping_rdd = sc.textFile(state_mapping_file)\n","\n","# Create a dictionary from the state mapping RDD\n","state_mapping_dict = dict(state_mapping_rdd\n","                          .map(lambda line: line.split(','))\n","                          .filter(lambda x: len(x) == 2) # Filter out lines that don't have two elements after the split\n","                          .collect())\n","\n","# Function to replace state names with state codes\n","def replace_state_name(citizen):\n","    fields = citizen.split(',')\n","    state_name = fields[4]  # Assuming state name is the 5th field\n","    state_code = state_mapping_dict.get(state_name, state_name)  # Get code or keep state name if not found\n","    fields[4] = state_code  # Replace state name with state code\n","    return ','.join(fields)\n","\n","# Replace state names in citizen data\n","compressed_citizen_data = citizen_rdd.map(replace_state_name)\n","\n","# Save the compressed result to a new file\n","compressed_citizen_data.saveAsTextFile(\"path_to_output_compressed_citizen.txt\")\n","\n","# Stop the SparkContext\n","sc.stop()"],"cell_type":"code","metadata":{"id":"f-4YqJt5-Ow6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","Create dataset (text file) with fields like ‘Student Name’, ‘Institute’, ‘Program Name’, and ‘Gender’ and solve following questions.\n","1. Compute number of students from each Institute.\n","2. Number of students enrolled to any program.\n","3. Number of ‘boy’ and ‘girl’ students.\n","4. Number of ‘boy’ and ‘girl’ students from selected Institute."],"metadata":{"id":"NVJ6zSzKFqBZ"}},{"cell_type":"code","source":["from pyspark import SparkContext\n","\n","# Initialize SparkContext\n","sc = SparkContext.getOrCreate()\n","\n","# Load the dataset from students.txt\n","data_file = \"/content/student.txt\"\n","students_rdd = sc.textFile(data_file)\n","\n","# Skip the header and split each line into fields\n","header = students_rdd.first()  # Get the header\n","students_data = students_rdd.filter(lambda line: line != header).map(lambda line: line.split(','))\n","\n","# 1. Compute number of students from each Institute\n","students_per_institute = students_data.map(lambda x: (x[1], 1))  # (Institute, 1)\n","students_count_per_institute = students_per_institute.reduceByKey(lambda a, b: a + b)\n","\n","print(\"Number of students from each Institute:\")\n","for institute, count in students_count_per_institute.collect():\n","    print(f\"{institute}: {count}\")\n","\n","# 2. Number of students enrolled in any program\n","total_students = students_data.count()\n","print(f\"\\nTotal number of students enrolled in any program: {total_students}\")\n","\n","# 3. Number of 'boy' and 'girl' students\n","gender_count = students_data.map(lambda x: (x[3].strip().lower(), 1))  # (Gender, 1)\n","gender_count_total = gender_count.reduceByKey(lambda a, b: a + b)\n","\n","print(\"\\nNumber of 'boy' and 'girl' students:\")\n","for gender, count in gender_count_total.collect():\n","    print(f\"{gender.capitalize()}: {count}\")\n","\n","# 4. Number of 'boy' and 'girl' students from a selected Institute\n","selected_institute = \"Institute A\"  # Change this to the desired institute\n","gender_count_selected_institute = students_data.filter(lambda x: x[1].strip() == selected_institute) \\\n","    .map(lambda x: (x[3].strip().lower(), 1))\n","\n","gender_count_selected_total = gender_count_selected_institute.reduceByKey(lambda a, b: a + b)\n","\n","print(f\"\\nNumber of 'boy' and 'girl' students from {selected_institute}:\")\n","for gender, count in gender_count_selected_total.collect():\n","    print(f\"{gender.capitalize()}: {count}\")\n","\n","# Stop the SparkContext\n","sc.stop()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ih0gPJlMFqMZ","executionInfo":{"status":"ok","timestamp":1727937494347,"user_tz":-330,"elapsed":3495,"user":{"displayName":"Smayan Hegde","userId":"00330384273200570673"}},"outputId":"4bcd1904-e479-42e7-aea1-5b039545ca93"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of students from each Institute:\n","Institute A: 4\n","Institute B: 3\n","Institute C: 3\n","\n","Total number of students enrolled in any program: 10\n","\n","Number of 'boy' and 'girl' students:\n","Female: 5\n","Male: 5\n","\n","Number of 'boy' and 'girl' students from Institute A:\n","Female: 1\n","Male: 3\n"]}]},{"cell_type":"markdown","source":["Dataset: Temperature of Indian Cities. Fields of dataset are Date, Average Temperature, City, Country, Latitude and Longitude (Use dataset attached to MapReduce assignment). Solve following questions\n","1. Find maximum and minimum temperature of all cities from the given dataset\n","2. Count number of data point for each city.\n","3. Find the maximum and minimum temperature for city Bangalore from the given dataset.\n","4. Find the maximum and minimum temperature for any given city from the given dataset. City name should be passed through command line argument."],"metadata":{"id":"Q1rEmhf7G4HD"}},{"cell_type":"code","source":["from pyspark import SparkContext\n","import sys\n","\n","# Initialize SparkContext\n","sc = SparkContext.getOrCreate()\n","\n","# Load the dataset from the file\n","data_file = \"/content/sampleTemp.txt\"  # Replace with the actual path to your dataset\n","temperature_rdd = sc.textFile(data_file)\n","\n","# Skip the header and split each line into fields\n","header = temperature_rdd.first()  # Get the header\n","temperature_data = temperature_rdd.filter(lambda line: line != header).map(lambda line: line.split(','))\n","\n","# Question 1: Find maximum and minimum temperature of all cities\n","# (City, Average Temperature)\n","city_temp = temperature_data.map(lambda x: (x[2], float(x[1])) if len(x) > 2 else ('Unknown', 0.0))  # Handle potential errors\n","max_temp_per_city = city_temp.reduceByKey(lambda a, b: max(a, b))\n","min_temp_per_city = city_temp.reduceByKey(lambda a, b: min(a, b))\n","\n","print(\"Maximum temperature of all cities:\")\n","for city, max_temp in max_temp_per_city.collect():\n","    print(f\"{city}: {max_temp}\")\n","\n","print(\"\\nMinimum temperature of all cities:\")\n","for city, min_temp in min_temp_per_city.collect():\n","    print(f\"{city}: {min_temp}\")\n","\n","# Question 2: Count number of data points for each city\n","data_points_per_city = temperature_data.map(lambda x: (x[2], 1) if len(x) > 2 else ('Unknown', 1)) # Handle potential errors\n","data_point_count = data_points_per_city.reduceByKey(lambda a, b: a + b)\n","\n","print(\"\\nNumber of data points for each city:\")\n","for city, count in data_point_count.collect():\n","    print(f\"{city}: {count}\")\n","\n","# Question 3: Find max and min temperature for Bangalore\n","# Convert city names to lowercase for case-insensitive comparison\n","bangalore_temps = temperature_data.filter(lambda x: x[2].strip().lower() == \"bangalore\" if len(x) > 2 else False)\n","\n","# Check if bangalore_temps is empty and handle it\n","if bangalore_temps.isEmpty():\n","    print(\"No data found for Bangalore\")\n","else:\n","    bangalore_max = bangalore_temps.map(lambda x: float(x[1])).max()\n","    bangalore_min = bangalore_temps.map(lambda x: float(x[1])).min()\n","    print(f\"\\nBangalore - Max Temperature: {bangalore_max}, Min Temperature: {bangalore_min}\")\n","\n","# Question 4: Find max and min temperature for any given city\n","# Removed the command-line argument check\n","given_city = input(\"Enter city name: \").strip().lower() # Get city name from user input\n","city_temps = temperature_data.filter(lambda x: x[2].strip().lower() == given_city if len(x) > 2 else False)\n","if city_temps.isEmpty():\n","  print(f\"No data found for city: {given_city}\")\n","\n","else:\n","  city_max = city_temps.map(lambda x: float(x[1])).max()\n","  city_min = city_temps.map(lambda x: float(x[1])).min()\n","  print(f\"{given_city.capitalize()} - Max Temperature: {city_max}, Min Temperature: {city_min}\")\n","\n","\n","# Stop the SparkContext\n","sc.stop()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fvIQWhKOG4sB","executionInfo":{"status":"ok","timestamp":1727941829794,"user_tz":-330,"elapsed":25429,"user":{"displayName":"Smayan Hegde","userId":"00330384273200570673"}},"outputId":"8ce34c18-61bf-408c-aa04-2d2393d3f782"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Maximum temperature of all cities:\n","Unknown: 0.0\n","\n","Minimum temperature of all cities:\n","Unknown: 0.0\n","\n","Number of data points for each city:\n","Unknown: 960547\n","No data found for Bangalore\n","Enter city name: Raipur\n","No data found for city: raipur\n"]}]},{"cell_type":"markdown","source":["Create dataset (text file) of bank transactions. Fields in file are ‘Bank ID’, ‘Account Number’, ‘Transaction Date’, ‘Transaction Type’ (credit or debit), ‘Transaction Amount’. Date format is dd-mm-yyyy.\n","1. Count unique number of customers\n","2. Count unique number of Bank ID\n","3. Count unique number of customers per Bank ID\n","4. Number of transactions for given Account Number\n","5. Number of credit transactions for given Account Number in a given year"],"metadata":{"id":"Sov0QddGW5bx"}},{"cell_type":"code","source":["from pyspark import SparkContext\n","import random\n","from datetime import datetime, timedelta\n","\n","# Initialize SparkContext\n","sc = SparkContext.getOrCreate()\n","\n","# Constants for dataset generation\n","NUM_TRANSACTIONS = 1000  # Number of transactions to generate\n","BANK_IDS = ['B001', 'B002', 'B003', 'B004', 'B005']  # Example bank IDs\n","ACCOUNT_NUMBERS = [f'ACC{random.randint(100000, 999999)}' for _ in range(200)]  # Example account numbers\n","TRANSACTION_TYPES = ['credit', 'debit']\n","start_date = datetime(2020, 1, 1)\n","end_date = datetime(2024, 12, 31)\n","\n","def random_date(start, end):\n","    return start + timedelta(days=random.randint(0, (end - start).days))\n","\n","# Generate random transactions\n","transactions = []\n","for _ in range(NUM_TRANSACTIONS):\n","    bank_id = random.choice(BANK_IDS)\n","    account_number = random.choice(ACCOUNT_NUMBERS)\n","    transaction_date = random_date(start_date, end_date).strftime('%d-%m-%Y')\n","    transaction_type = random.choice(TRANSACTION_TYPES)\n","    transaction_amount = round(random.uniform(10, 5000), 2)  # Random amount between 10 and 5000\n","    transactions.append((bank_id, account_number, transaction_date, transaction_type, transaction_amount))\n","\n","# Create RDD from the transactions list\n","rdd = sc.parallelize(transactions)\n","\n","# Read the input file into an RDD\n","rdd = sc.textFile('/content/bank.txt').map(lambda line: line.split('\\t'))\n","\n","# Save RDD to text file\n","output_file = 'bank_transactions.txt'\n","rdd.map(lambda x: '\\t'.join(map(str, x))).saveAsTextFile(output_file)\n","\n","# Counting functions\n","def count_unique_customers(rdd):\n","    return rdd.map(lambda x: x[1]).distinct().count()\n","\n","def count_unique_bank_ids(rdd):\n","    return rdd.map(lambda x: x[0]).distinct().count()\n","\n","def count_customers_per_bank_id(rdd):\n","    return rdd.map(lambda x: (x[0], x[1])).distinct().groupByKey().mapValues(len)\n","\n","def count_transactions_for_account(rdd, account_number):\n","    return rdd.filter(lambda x: x[1] == account_number).count()\n","\n","def count_credit_transactions_in_year(rdd, account_number, year):\n","    return rdd.filter(lambda x: x[1] == account_number and\n","                     x[3] == 'credit' and\n","                     datetime.strptime(x[2], '%d-%m-%Y').year == year).count()\n","\n","# Example usage of counting functions\n","print(\"Unique Customers:\", count_unique_customers(rdd))\n","print(\"Unique Bank IDs:\", count_unique_bank_ids(rdd))\n","print(\"Customers per Bank ID:\\n\", count_customers_per_bank_id(rdd).collect())\n","\n","# Example account number and year for counting transactions\n","example_account_number = rdd.map(lambda x: x[1]).distinct().take(1)[0]  # Get a random account number from the dataset\n","example_year = 2022  # Specify a year to check credit transactions\n","print(f\"Transactions for Account Number '{example_account_number}':\", count_transactions_for_account(rdd, example_account_number))\n","print(f\"Credit Transactions for Account Number '{example_account_number}' in {example_year}:\",\n","      count_credit_transactions_in_year(rdd, example_account_number, example_year))\n","\n","# Stop the SparkContext\n","sc.stop()\n"],"metadata":{"id":"oogSZvFcJYBj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727942055292,"user_tz":-330,"elapsed":6031,"user":{"displayName":"Smayan Hegde","userId":"00330384273200570673"}},"outputId":"28366124-08f0-4ec4-fc90-4bc321f72eb5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Unique Customers: 22\n","Unique Bank IDs: 3\n","Customers per Bank ID:\n"," [('CAN00123', 8), ('SBI00042', 12), ('ICI00072', 6)]\n","Transactions for Account Number '123876232': 1\n","Credit Transactions for Account Number '123876232' in 2022: 0\n"]}]}]}